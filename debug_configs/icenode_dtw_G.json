{
    "emb": {
        "diag": {
            "glove_config": {
                "iterations": 30,
                "window_size_days": 730
            },
            "embeddings_dim": 200,
            "attention_method": "tanh",
            "attention_dim": 150
	},
        "kind": "glove_gram"
    },
    "model": {
        "distance": "bce",
        "ode_dyn": "mlp2",
        "ode_init_var": 1.1209976088140297e-07,
        "ode_with_bias": true,
        "sdtw_gamma": 0.001,
        "state_size": 30,
        "timescale": 7,
        "trajectory_samples": 7
    },
    "training": {
        "batch_size": 32,
        "decay_rate": 0.7,
        "epochs": 100,
        "loss_mixing": {
            "L_dyn": 1e3,
            "L_l1": 0,
            "L_l2": 0
        },
        "lr": 1e-3,
        "optimizer": "adam"
    }
}
