{
    "emb": {
        "dx": {
           "decoder_n_layers": 2,
            "classname": "CachedGRAM", 
            "attention_size": 100,
            "attention_method": "tanh",
            "embeddings_size": 150,
            "glove_config": {
                "iterations": 2,
                "window_size_days": 730
            }
        }
    },
    "model": {
        "state_size": 350
    },
    "training": {
        "batch_size": 17,
        "decay_rate": null,
        "epochs": 2,
        "loss_mixing": {
            "L_l1": 0,
            "L_l2": 0
        },
        "lr": 0.0010161679708784964,
        "opt": "adam"
    }
}
