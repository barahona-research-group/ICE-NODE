{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d59416-2f91-473a-8103-a3cd6343ac3d",
   "metadata": {},
   "source": [
    "## Libs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c28bbb-0bea-4165-9bee-f54dad24ccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom \n",
    "import equinox as eqx\n",
    "import optax\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "import sklearn.neural_network as sknn\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa\n",
    "from sklearn.impute import IterativeImputer, KNNImputer, SimpleImputer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "# jax.config.update('jax_check_tracer_leaks', True) \n",
    "sys.path.append(\"../../..\")\n",
    "from lib.ml.base_models import ICNNObsDecoder\n",
    "import lib.ehr.example_datasets.mimiciv_aki as m4aki\n",
    "from lib.ehr.tvx_ehr import TVxEHR\n",
    "from lib.utils import modified_environ, write_config\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bedd275f-c187-456f-9624-228e685e2531",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa92a456-7044-4d3a-bfbb-a9fcbe3079fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rng = np.random.RandomState(42)\n",
    "\n",
    "X_diabetes, y_diabetes = load_diabetes(return_X_y=True)\n",
    "X_california, y_california = fetch_california_housing(return_X_y=True)\n",
    "X_california = X_california[:5000]\n",
    "y_california = y_california[:5000]\n",
    "X_diabetes = X_diabetes[:5000]\n",
    "y_diabetes = y_diabetes[:5000]\n",
    "\n",
    "\n",
    "def add_missing_values(X_full, y_full):\n",
    "    n_samples, n_features = X_full.shape\n",
    "\n",
    "    # Add missing values in 75% of the lines\n",
    "    missing_rate = 0.75\n",
    "    n_missing_samples = int(n_samples * missing_rate)\n",
    "\n",
    "    missing_samples = np.zeros(n_samples, dtype=bool)\n",
    "    missing_samples[:n_missing_samples] = True\n",
    "\n",
    "    rng.shuffle(missing_samples)\n",
    "    missing_features = rng.randint(0, n_features, n_missing_samples)\n",
    "    X_missing = X_full.copy()\n",
    "    X_missing[missing_samples, missing_features] = np.nan\n",
    "    y_missing = y_full.copy()\n",
    "\n",
    "    return X_missing, y_missing\n",
    "\n",
    "\n",
    "X_miss_california, y_miss_california = add_missing_values(X_california, y_california)\n",
    "X_miss_diabetes, y_miss_diabetes = add_missing_values(X_diabetes, y_diabetes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edced3f-c431-48ee-af88-8d274a3fd49f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5000, 8), (442, 10))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_miss_california.shape, X_miss_diabetes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e678fa0-e9a7-4120-aba9-5c17792af3d7",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed9fabd6-6809-460f-a454-9572153c8299",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.RandomState(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3c064fd5-4daa-4c35-9760-0af20b291884",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal, Callable, Tuple\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import equinox as eqx\n",
    "import jax.nn as jnn\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jr\n",
    "import optax\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def r_squared(y, y_hat):\n",
    "    y = y.squeeze()\n",
    "    y_hat = y_hat.squeeze()\n",
    "    \n",
    "    y_bar = y.mean()\n",
    "    ss_tot = ((y-y_bar)**2).sum()\n",
    "    ss_res = ((y-y_hat)**2).sum()\n",
    "    return 1 - (ss_res/ss_tot)\n",
    "\n",
    "\n",
    "class MLPRegressor(eqx.Module):\n",
    "    mlp: eqx.nn.MLP\n",
    "    optimiser: optax.GradientTransformation = eqx.static_field()\n",
    "    lr: float = eqx.static_field()\n",
    "    alpha: float = eqx.static_field()\n",
    "    loss_fn: Callable[[jnp.ndarray], jnp.ndarray] = eqx.static_field()\n",
    "    batch_size: int = eqx.static_field()\n",
    "    max_iters: int = eqx.static_field()\n",
    "    seed: int = eqx.static_field()\n",
    "\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, depth: int,\n",
    "                 activation: Literal['relu', 'tanh', 'sigmoid', 'elu', 'softplus', 'leaky_relu'], batch_size: int,\n",
    "                 lr: float = 1e-3, alpha: float = 1e-5, loss_fn: Literal['mse', 'mae'] = 'mse',\n",
    "                 max_iters: int = 100,\n",
    "                 optax_optimiser = optax.adam,\n",
    "                 seed: int = 0):\n",
    "        super().__init__()\n",
    "        if activation == 'relu':\n",
    "            activation_f = jnn.relu\n",
    "        elif activation == 'tanh':\n",
    "            activation_f = jnn.tanh\n",
    "        elif activation == 'sigmoid':\n",
    "            activation_f = jnn.sigmoid\n",
    "        elif activation == 'softplus':\n",
    "            activation_f = jnn.softplus\n",
    "        elif activation == 'elu':\n",
    "            activation_f = jnn.elu\n",
    "        elif activation == 'leaky_relu':\n",
    "            activation_f = lambda x: jnn.leaky_relu(x, negative_slope=0.01)\n",
    "        else:\n",
    "            activation_f = lambda x: x\n",
    "\n",
    "        if loss_fn == 'mse':\n",
    "            self.loss_fn = jnp.square\n",
    "        elif loss_fn == 'mae':\n",
    "            self.loss_fn = jnp.abs\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown loss function {loss_fn}\")\n",
    "\n",
    "        self.mlp = eqx.nn.MLP(input_size, output_size, hidden_size, depth, activation=activation_f,\n",
    "                              key=jr.PRNGKey(seed),\n",
    "                              use_bias=True, use_final_bias=True)\n",
    "        self.optimiser = optax_optimiser(lr)\n",
    "        self.lr = lr\n",
    "        self.alpha = alpha\n",
    "        self.batch_size = batch_size\n",
    "        self.max_iters = max_iters\n",
    "        self.seed = seed\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def loss(self, batch_X: jnp.ndarray, batch_y: jnp.ndarray) -> jnp.ndarray:\n",
    "        y_hat = eqx.filter_vmap(self.predict)(batch_X)\n",
    "        return jnp.mean(self.loss_fn(y_hat.flatten() - batch_y.flatten()))\n",
    "\n",
    "    @property\n",
    "    def weights_list(self):\n",
    "        return list(l.weight for l in self.mlp.layers)\n",
    "\n",
    "    @eqx.filter_value_and_grad\n",
    "    def loss_grad(self, batch_X: jnp.ndarray, batch_y: jnp.ndarray):\n",
    "        L = self.loss(batch_X, batch_y)\n",
    "        if self.alpha > 0:\n",
    "            L += self.alpha * sum(jnp.sum(jnp.square(w)) for w in self.weights_list) / (2 * len(batch_X))\n",
    "        return L\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def make_step(self, opt_state, batch_X: jnp.ndarray, batch_y: jnp.ndarray) -> Tuple[\n",
    "        jnp.ndarray, \"MLPRegressor\", optax.OptState]:\n",
    "        loss, grads = self.loss_grad(batch_X, batch_y)\n",
    "        updates, opt_state = self.optimiser.update(grads, opt_state,\n",
    "                                                   params=eqx.filter(self, eqx.is_inexact_array),\n",
    "                                                   value=loss, grad=grads, \n",
    "                                                   value_fn=lambda m: eqx.combine(m, self).loss(batch_X, batch_y))\n",
    "        return loss, eqx.apply_updates(self, updates), opt_state\n",
    "\n",
    "    @staticmethod\n",
    "    def dataloader(arrays, batch_size, *, key):\n",
    "        dataset_size = arrays[0].shape[0]\n",
    "        indices = jnp.arange(dataset_size)\n",
    "        while True:\n",
    "            perm = jr.permutation(key, indices)\n",
    "            (key, _) = jr.split(key, 2)\n",
    "            start = 0\n",
    "            end = batch_size\n",
    "            while end < dataset_size:\n",
    "                batch_perm = perm[start:end]\n",
    "                yield tuple(array[batch_perm] for array in arrays)\n",
    "                start = end\n",
    "                end = start + batch_size\n",
    "\n",
    "    def fit(self, X: jnp.ndarray, y: jnp.ndarray):\n",
    "        model = self\n",
    "        opt_state = self.optimiser.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "        train_batches = self.dataloader((X, y), self.batch_size, key=jr.PRNGKey(self.seed))\n",
    "        progress = tqdm(range(self.max_iters), leave=False)\n",
    "        for step, (batch_X, batch_y) in zip(progress, train_batches):\n",
    "            loss, model, opt_state = model.make_step(opt_state, batch_X, batch_y)\n",
    "            r2 = r_squared(batch_y, model.predict(batch_X))\n",
    "            progress.set_description(f\"Loss: {loss:.3f}, R2: {r2:.3f}\")\n",
    "\n",
    "        return model\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def predict(self, X: jnp.ndarray):\n",
    "        if jnp.ndim(X) == 1:\n",
    "            X = X[None, :]\n",
    "        return eqx.filter_vmap(self.mlp)(X)\n",
    "\n",
    "\n",
    "class MLPRegressorICNN(MLPRegressor):\n",
    "    imputer: ICNNObsDecoder\n",
    "\n",
    "    def __init__(self, input_size: int, output_size: int, hidden_size: int, depth: int,\n",
    "                 activation: Literal['relu', 'tanh', 'sigmoid'], batch_size: int,\n",
    "                 imputer_hidden_size_multiplier: int = 3, imputer_depth: int = 5,\n",
    "                 lr: float = 1e-3, alpha: float = 1e-5, loss_fn: Literal['mse', 'mae'] = 'mse',\n",
    "                 max_iters: int = 100,\n",
    "                 optax_optimiser = optax.adam,\n",
    "                 icnn_optax_optimiser_name = 'adam',\n",
    "                 seed: int = 0):\n",
    "        super().__init__(input_size, output_size, hidden_size, depth, activation, batch_size, lr, alpha, loss_fn,\n",
    "                         max_iters, optax_optimiser, seed)\n",
    "        self.imputer = ICNNObsDecoder(observables_size=input_size, state_size=0,\n",
    "                                      hidden_size_multiplier=imputer_hidden_size_multiplier,\n",
    "                                      depth=imputer_depth, \n",
    "                                      optax_optimiser_name=icnn_optax_optimiser_name,\n",
    "                                      key=jr.PRNGKey(seed))\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def impute(self, X: jnp.ndarray):\n",
    "        if jnp.ndim(X) == 1:\n",
    "            X = X[None, :]\n",
    "        M = jnp.where(jnp.isnan(X), False, True)\n",
    "        X = jnp.nan_to_num(X)\n",
    "        X, aux = eqx.filter_vmap(self.imputer.partial_input_optimise)(X, M)\n",
    "        return X\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def predict(self, X: jnp.ndarray):\n",
    "        if jnp.ndim(X) == 1:\n",
    "            X = X[None, :]\n",
    "        X = self.impute(X)\n",
    "        return eqx.filter_vmap(self.mlp)(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ef1ac49-9b42-4a3a-8f56-86c9b38de838",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_mlp_regressor(X, opt=optax.adam):\n",
    "    return MLPRegressor(input_size=X.shape[1],\n",
    "                        output_size=1, hidden_size=X.shape[1]*5,\n",
    "                        optax_optimiser=opt,\n",
    "                        alpha=0., depth=2, activation='leaky_relu', batch_size=32, max_iters=1500, lr=1e-3)\n",
    "\n",
    "\n",
    "def get_sklearn_mlp_regressor(X):\n",
    "    return sknn.MLPRegressor(hidden_layer_sizes=(X.shape[1]*5, )*2,\n",
    "                             solver='lbfgs',\n",
    "                             alpha=0., activation='relu', batch_size=32, max_iter=1500, learning_rate_init=1e-3)\n",
    "\n",
    "def get_mlp_icnn_regressor(X, icnn_optax_optimiser_name, opt=optax.adam):\n",
    "    return MLPRegressorICNN(input_size=X.shape[1],\n",
    "                            output_size=1, hidden_size=X.shape[1]*5,                       \n",
    "                            optax_optimiser=opt,\n",
    "                            icnn_optax_optimiser_name=icnn_optax_optimiser_name,\n",
    "                            alpha=0., depth=2, activation='leaky_relu', batch_size=32, max_iters=5, lr=1e-3)\n",
    "\n",
    "\n",
    "def get_rf_regressor(X = None):\n",
    "    return RandomForestRegressor(random_state=0)\n",
    "\n",
    "\n",
    "\n",
    "def get_impute_regress_scores(model, X_missing_train, y_train, X_missing_test, y_test):\n",
    "    scaler = StandardScaler()\n",
    "    scaler = scaler.fit(X_missing_train)\n",
    "    X_missing_train = scaler.transform(X_missing_train)\n",
    "    X_missing_test = scaler.transform(X_missing_test)\n",
    "    if isinstance(model, tuple):\n",
    "        model, imputer = model\n",
    "        imputer = imputer.fit(X_missing_train)\n",
    "        X_missing_train = imputer.transform(X_missing_train)\n",
    "        X_missing_test = imputer.transform(X_missing_test)\n",
    "        model = model.fit(X_missing_train, y_train)\n",
    "        ret_model = (model, imputer)\n",
    "    else:\n",
    "        model = model.fit(X_missing_train, y_train)\n",
    "        ret_model = model\n",
    "\n",
    "    return ({'MSE': np.mean(np.square(model.predict(X_missing_train).flatten() - y_train.flatten())), \n",
    "             'R2': r_squared(y_train, model.predict(X_missing_train))},\n",
    "            {'MSE': np.mean(np.square(model.predict(X_missing_test).flatten() - y_test.flatten())), \n",
    "             'R2': r_squared(y_test, model.predict(X_missing_test))}), ret_model\n",
    "\n",
    "def get_imput_regress_scores_cv(model, X, y, k=5):\n",
    "    kf = KFold(n_splits=k, shuffle=True, random_state=0)\n",
    "    train_mse, train_r2 = [], []\n",
    "    test_mse, test_r2 = [], []\n",
    "    \n",
    "    for i, (train_index, test_index) in enumerate(kf.split(X)):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        (train_scores, test_scores), model = get_impute_regress_scores(model, X_train, y_train, X_test, y_test)\n",
    "        train_mse.append(train_scores['MSE'])\n",
    "        train_r2.append(train_scores['R2'])\n",
    "        test_mse.append(test_scores['MSE'])\n",
    "        test_r2.append(test_scores['R2'])\n",
    "    \n",
    "\n",
    "    return pd.DataFrame({'split': ['TRAIN'] * k +  ['TEST'] * k,\n",
    "                        'MSE': train_mse + test_mse,\n",
    "                        'R2':  train_r2 + test_r2,\n",
    "                        'Fold': list(range(k)) * 2})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bb36cd0-60a4-441a-8bb1-e37455e8e7a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "optax_optimisers = {\n",
    "    'adam': optax.adam,\n",
    "    'polyak_sgd': optax.polyak_sgd,\n",
    "    'novograd': optax.novograd,\n",
    "    'lamb': optax.lamb,\n",
    "    'yogi': optax.yogi,\n",
    "}\n",
    "\n",
    "\n",
    "imputers =  {\n",
    "    'zero_imputer': SimpleImputer(missing_values=np.nan, add_indicator=False, strategy=\"constant\", fill_value=0),\n",
    "    'mean_imputer': SimpleImputer(missing_values=np.nan, add_indicator=False, strategy=\"mean\", fill_value=0),\n",
    "    'knn_imputer': KNNImputer(missing_values=np.nan),\n",
    "    'iter_imputer': IterativeImputer(\n",
    "        missing_values=np.nan,\n",
    "        add_indicator=False,\n",
    "        random_state=0,\n",
    "        n_nearest_features=5,\n",
    "        max_iter=5,\n",
    "        sample_posterior=True,\n",
    "    )\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0dd67c92-498e-45cc-9526-7c7c145dda25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = []\n",
    "# for (ds_name, (X, y)) in zip(['California',\n",
    "#                              'Diabetes'\n",
    "#                              ],  [(X_miss_california, y_miss_california), \n",
    "#                                   (X_miss_diabetes, y_miss_diabetes)\n",
    "#                                  ]):\n",
    "\n",
    "#     for imputer_name, imputer in imputers.items():\n",
    "#         for model_name, model in [('SKL_MLP', get_sklearn_mlp_regressor(X)), ('SKL_RF', get_rf_regressor())]:\n",
    "#             res = get_imput_regress_scores_cv((model,imputer), X, y)\n",
    "#             res['Dataset'] = ds_name\n",
    "#             res['Imputer'] = imputer_name\n",
    "#             res['Model'] = model_name\n",
    "#             dfs.append(res)\n",
    "\n",
    "#         for opt_name, optimizer in optax_optimisers.items():\n",
    "#             model_name = 'JAX_MLP'\n",
    "#             model = get_mlp_regressor(X, opt=optimizer)\n",
    "#             res = get_imput_regress_scores_cv((model,imputer), X, y)\n",
    "#             res['Dataset'] = ds_name\n",
    "#             res['Imputer'] = imputer_name\n",
    "#             res['Model'] = model_name\n",
    "#             res['Optimizer'] = opt_name\n",
    "#             dfs.append(res)\n",
    "\n",
    "# pd.concat(dfs).to_csv('feedforward_imputations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "639280b1-42de-4d83-a8a7-03a5ad35bcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adam\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002468109130859375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0022897720336914062,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0023474693298339844,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0022995471954345703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002344369888305664,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polyak_sgd\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0025072097778320312,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.002454996109008789,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0025017261505126953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0022885799407958984,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0022356510162353516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "novograd\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0022416114807128906,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 36,
       "postfix": null,
       "prefix": "",
       "rate": null,
       "total": 5,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "139f8ae5ff9645b3b250000b32fb450c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(icnn_opt_name)\n\u001b[1;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m get_mlp_icnn_regressor(X, opt\u001b[38;5;241m=\u001b[39moptimizer, icnn_optax_optimiser_name\u001b[38;5;241m=\u001b[39micnn_opt_name)\n\u001b[0;32m---> 11\u001b[0m resicnn \u001b[38;5;241m=\u001b[39m get_imput_regress_scores_cv(model, X, y)\n\u001b[1;32m     12\u001b[0m resicnn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m ds_name\n\u001b[1;32m     13\u001b[0m resicnn[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m model_name\n",
      "Cell \u001b[0;32mIn[10], line 55\u001b[0m, in \u001b[0;36mget_imput_regress_scores_cv\u001b[0;34m(model, X, y, k)\u001b[0m\n\u001b[1;32m     53\u001b[0m X_train, X_test \u001b[38;5;241m=\u001b[39m X[train_index], X[test_index]\n\u001b[1;32m     54\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m y[train_index], y[test_index]\n\u001b[0;32m---> 55\u001b[0m (train_scores, test_scores), model \u001b[38;5;241m=\u001b[39m get_impute_regress_scores(model, X_train, y_train, X_test, y_test)\n\u001b[1;32m     56\u001b[0m train_mse\u001b[38;5;241m.\u001b[39mappend(train_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     57\u001b[0m train_r2\u001b[38;5;241m.\u001b[39mappend(train_scores[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "Cell \u001b[0;32mIn[10], line 39\u001b[0m, in \u001b[0;36mget_impute_regress_scores\u001b[0;34m(model, X_missing_train, y_train, X_missing_test, y_test)\u001b[0m\n\u001b[1;32m     37\u001b[0m     ret_model \u001b[38;5;241m=\u001b[39m (model, imputer)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mfit(X_missing_train, y_train)\n\u001b[1;32m     40\u001b[0m     ret_model \u001b[38;5;241m=\u001b[39m model\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39msquare(model\u001b[38;5;241m.\u001b[39mpredict(X_missing_train)\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m-\u001b[39m y_train\u001b[38;5;241m.\u001b[39mflatten())), \n\u001b[1;32m     43\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m'\u001b[39m: r_squared(y_train, model\u001b[38;5;241m.\u001b[39mpredict(X_missing_train))},\n\u001b[1;32m     44\u001b[0m         {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE\u001b[39m\u001b[38;5;124m'\u001b[39m: np\u001b[38;5;241m.\u001b[39mmean(np\u001b[38;5;241m.\u001b[39msquare(model\u001b[38;5;241m.\u001b[39mpredict(X_missing_test)\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m-\u001b[39m y_test\u001b[38;5;241m.\u001b[39mflatten())), \n\u001b[1;32m     45\u001b[0m          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2\u001b[39m\u001b[38;5;124m'\u001b[39m: r_squared(y_test, model\u001b[38;5;241m.\u001b[39mpredict(X_missing_test))}), ret_model\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 118\u001b[0m, in \u001b[0;36mMLPRegressor.fit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    116\u001b[0m progress \u001b[38;5;241m=\u001b[39m tqdm(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_iters), leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step, (batch_X, batch_y) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(progress, train_batches):\n\u001b[0;32m--> 118\u001b[0m     loss, model, opt_state \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mmake_step(opt_state, batch_X, batch_y)\n\u001b[1;32m    119\u001b[0m     r2 \u001b[38;5;241m=\u001b[39m r_squared(batch_y, model\u001b[38;5;241m.\u001b[39mpredict(batch_X))\n\u001b[1;32m    120\u001b[0m     progress\u001b[38;5;241m.\u001b[39mset_description(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, R2: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mr2\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/GP/env/icenode-dev/lib/python3.11/site-packages/equinox/_module.py:1189\u001b[0m, in \u001b[0;36mPartial.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1177\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Call the wrapped `self.func`.\u001b[39;00m\n\u001b[1;32m   1178\u001b[0m \n\u001b[1;32m   1179\u001b[0m \u001b[38;5;124;03m    **Arguments:**\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1187\u001b[0m \u001b[38;5;124;03m    The result of the wrapped function.\u001b[39;00m\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1189\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeywords)\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 90\u001b[0m, in \u001b[0;36mMLPRegressor.make_step\u001b[0;34m(self, opt_state, batch_X, batch_y)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_jit\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_step\u001b[39m(\u001b[38;5;28mself\u001b[39m, opt_state, batch_X: jnp\u001b[38;5;241m.\u001b[39mndarray, batch_y: jnp\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\n\u001b[1;32m     89\u001b[0m     jnp\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMLPRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m, optax\u001b[38;5;241m.\u001b[39mOptState]:\n\u001b[0;32m---> 90\u001b[0m     loss, grads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_grad(batch_X, batch_y)\n\u001b[1;32m     91\u001b[0m     updates, opt_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimiser\u001b[38;5;241m.\u001b[39mupdate(grads, opt_state,\n\u001b[1;32m     92\u001b[0m                                                params\u001b[38;5;241m=\u001b[39meqx\u001b[38;5;241m.\u001b[39mfilter(\u001b[38;5;28mself\u001b[39m, eqx\u001b[38;5;241m.\u001b[39mis_inexact_array),\n\u001b[1;32m     93\u001b[0m                                                value\u001b[38;5;241m=\u001b[39mloss, grad\u001b[38;5;241m=\u001b[39mgrads, \n\u001b[1;32m     94\u001b[0m                                                value_fn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m m: eqx\u001b[38;5;241m.\u001b[39mcombine(m, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mloss(batch_X, batch_y))\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, eqx\u001b[38;5;241m.\u001b[39mapply_updates(\u001b[38;5;28mself\u001b[39m, updates), opt_state\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 82\u001b[0m, in \u001b[0;36mMLPRegressor.loss_grad\u001b[0;34m(self, batch_X, batch_y)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_value_and_grad\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss_grad\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_X: jnp\u001b[38;5;241m.\u001b[39mndarray, batch_y: jnp\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[0;32m---> 82\u001b[0m     L \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss(batch_X, batch_y)\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     84\u001b[0m         L \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39malpha \u001b[38;5;241m*\u001b[39m \u001b[38;5;28msum\u001b[39m(jnp\u001b[38;5;241m.\u001b[39msum(jnp\u001b[38;5;241m.\u001b[39msquare(w)) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights_list) \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_X))\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 73\u001b[0m, in \u001b[0;36mMLPRegressor.loss\u001b[0;34m(self, batch_X, batch_y)\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_jit\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mloss\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch_X: jnp\u001b[38;5;241m.\u001b[39mndarray, batch_y: jnp\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m jnp\u001b[38;5;241m.\u001b[39mndarray:\n\u001b[0;32m---> 73\u001b[0m     y_hat \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mfilter_vmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict)(batch_X)\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fn(y_hat\u001b[38;5;241m.\u001b[39mflatten() \u001b[38;5;241m-\u001b[39m batch_y\u001b[38;5;241m.\u001b[39mflatten()))\n",
      "    \u001b[0;31m[... skipping hidden 21 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 163\u001b[0m, in \u001b[0;36mMLPRegressorICNN.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mndim(X) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    162\u001b[0m     X \u001b[38;5;241m=\u001b[39m X[\u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[0;32m--> 163\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimpute(X)\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m eqx\u001b[38;5;241m.\u001b[39mfilter_vmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp)(X)\n",
      "    \u001b[0;31m[... skipping hidden 16 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[5], line 156\u001b[0m, in \u001b[0;36mMLPRegressorICNN.impute\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    154\u001b[0m M \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mwhere(jnp\u001b[38;5;241m.\u001b[39misnan(X), \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    155\u001b[0m X \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mnan_to_num(X)\n\u001b[0;32m--> 156\u001b[0m X, aux \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mfilter_vmap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimputer\u001b[38;5;241m.\u001b[39mpartial_input_optimise)(X, M)\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "    \u001b[0;31m[... skipping hidden 21 frame]\u001b[0m\n",
      "File \u001b[0;32m~/GP/ICENODE/notebooks/mimic_icu/exploratory/../../../lib/ml/base_models.py:610\u001b[0m, in \u001b[0;36mICNNObsDecoder.partial_input_optimise\u001b[0;34m(self, input, fixed_mask)\u001b[0m\n\u001b[1;32m    608\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_jit\n\u001b[1;32m    609\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpartial_input_optimise\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: jnp\u001b[38;5;241m.\u001b[39mndarray, fixed_mask: jnp\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[jnp\u001b[38;5;241m.\u001b[39mndarray, ImputerMetrics]:\n\u001b[0;32m--> 610\u001b[0m     sol \u001b[38;5;241m=\u001b[39m optx\u001b[38;5;241m.\u001b[39mminimise(\u001b[38;5;28;01mlambda\u001b[39;00m y, args: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mf_energy(y),\n\u001b[1;32m    611\u001b[0m                         solver\u001b[38;5;241m=\u001b[39moptx\u001b[38;5;241m.\u001b[39mBestSoFarMinimiser(solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptax_solver(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptax_optimiser_name)),\n\u001b[1;32m    612\u001b[0m                         adjoint\u001b[38;5;241m=\u001b[39moptx\u001b[38;5;241m.\u001b[39mImplicitAdjoint(linear_solver\u001b[38;5;241m=\u001b[39mlineax\u001b[38;5;241m.\u001b[39mAutoLinearSolver(well_posed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)),\n\u001b[1;32m    613\u001b[0m                         max_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m    614\u001b[0m                         options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mdict\u001b[39m(fixed_mask\u001b[38;5;241m=\u001b[39mfixed_mask),\n\u001b[1;32m    615\u001b[0m                         y0\u001b[38;5;241m=\u001b[39m\u001b[38;5;28minput\u001b[39m, throw\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    616\u001b[0m     num_steps \u001b[38;5;241m=\u001b[39m sol\u001b[38;5;241m.\u001b[39mstats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_steps\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m sol\u001b[38;5;241m.\u001b[39mvalue, ImputerMetrics(n_steps\u001b[38;5;241m=\u001b[39mjnp\u001b[38;5;241m.\u001b[39marray(num_steps))\n",
      "    \u001b[0;31m[... skipping hidden 15 frame]\u001b[0m\n",
      "File \u001b[0;32m~/GP/env/icenode-dev/lib/python3.11/site-packages/optimistix/_minimise.py:114\u001b[0m, in \u001b[0;36mminimise\u001b[0;34m(fn, solver, y0, args, options, has_aux, max_steps, adjoint, throw, tags)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(f_struct, jax\u001b[38;5;241m.\u001b[39mShapeDtypeStruct)\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m f_struct\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m ()\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39missubdtype(f_struct\u001b[38;5;241m.\u001b[39mdtype, jnp\u001b[38;5;241m.\u001b[39mfloating)\n\u001b[1;32m    109\u001b[0m ):\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    111\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimisation function must output a single floating-point scalar.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    112\u001b[0m     )\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m iterative_solve(\n\u001b[1;32m    115\u001b[0m     fn,\n\u001b[1;32m    116\u001b[0m     solver,\n\u001b[1;32m    117\u001b[0m     y0,\n\u001b[1;32m    118\u001b[0m     args,\n\u001b[1;32m    119\u001b[0m     options,\n\u001b[1;32m    120\u001b[0m     max_steps\u001b[38;5;241m=\u001b[39mmax_steps,\n\u001b[1;32m    121\u001b[0m     adjoint\u001b[38;5;241m=\u001b[39madjoint,\n\u001b[1;32m    122\u001b[0m     throw\u001b[38;5;241m=\u001b[39mthrow,\n\u001b[1;32m    123\u001b[0m     tags\u001b[38;5;241m=\u001b[39mtags,\n\u001b[1;32m    124\u001b[0m     aux_struct\u001b[38;5;241m=\u001b[39maux_struct,\n\u001b[1;32m    125\u001b[0m     f_struct\u001b[38;5;241m=\u001b[39mf_struct,\n\u001b[1;32m    126\u001b[0m     rewrite_fn\u001b[38;5;241m=\u001b[39m_rewrite_fn,\n\u001b[1;32m    127\u001b[0m )\n",
      "File \u001b[0;32m~/GP/env/icenode-dev/lib/python3.11/site-packages/optimistix/_iterate.py:346\u001b[0m, in \u001b[0;36miterative_solve\u001b[0;34m(fn, solver, y0, args, options, max_steps, adjoint, throw, tags, f_struct, aux_struct, rewrite_fn)\u001b[0m\n\u001b[1;32m    334\u001b[0m aux_struct \u001b[38;5;241m=\u001b[39m jtu\u001b[38;5;241m.\u001b[39mtree_map(eqxi\u001b[38;5;241m.\u001b[39mStatic, aux_struct)\n\u001b[1;32m    335\u001b[0m inputs \u001b[38;5;241m=\u001b[39m fn, solver, y0, args, options, max_steps, f_struct, aux_struct, tags\n\u001b[1;32m    336\u001b[0m (\n\u001b[1;32m    337\u001b[0m     out,\n\u001b[1;32m    338\u001b[0m     (\n\u001b[1;32m    339\u001b[0m         num_steps,\n\u001b[1;32m    340\u001b[0m         result,\n\u001b[1;32m    341\u001b[0m         dynamic_final_state,\n\u001b[1;32m    342\u001b[0m         static_state,\n\u001b[1;32m    343\u001b[0m         aux,\n\u001b[1;32m    344\u001b[0m         stats,\n\u001b[1;32m    345\u001b[0m     ),\n\u001b[0;32m--> 346\u001b[0m ) \u001b[38;5;241m=\u001b[39m adjoint\u001b[38;5;241m.\u001b[39mapply(_iterate, rewrite_fn, inputs, tags)\n\u001b[1;32m    347\u001b[0m final_state \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mcombine(dynamic_final_state, unwrap_jaxpr(static_state\u001b[38;5;241m.\u001b[39mvalue))\n\u001b[1;32m    348\u001b[0m stats \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m: num_steps, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_steps\u001b[39m\u001b[38;5;124m\"\u001b[39m: max_steps, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mstats}\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/GP/env/icenode-dev/lib/python3.11/site-packages/optimistix/_adjoint.py:148\u001b[0m, in \u001b[0;36mImplicitAdjoint.apply\u001b[0;34m(self, primal_fn, rewrite_fn, inputs, tags)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, primal_fn, rewrite_fn, inputs, tags):\n\u001b[1;32m    147\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs \u001b[38;5;241m+\u001b[39m (ft\u001b[38;5;241m.\u001b[39mpartial(eqxi\u001b[38;5;241m.\u001b[39mwhile_loop, kind\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlax\u001b[39m\u001b[38;5;124m\"\u001b[39m),)\n\u001b[0;32m--> 148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m implicit_jvp(primal_fn, rewrite_fn, inputs, tags, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear_solver)\n",
      "File \u001b[0;32m~/GP/env/icenode-dev/lib/python3.11/site-packages/optimistix/_ad.py:72\u001b[0m, in \u001b[0;36mimplicit_jvp\u001b[0;34m(fn_primal, fn_rewrite, inputs, tags, linear_solver)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _is_global_function(fn_primal)\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m _is_global_function(fn_rewrite)\n\u001b[0;32m---> 72\u001b[0m root, residual \u001b[38;5;241m=\u001b[39m _implicit_impl(fn_primal, fn_rewrite, inputs, tags, linear_solver)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m root, jtu\u001b[38;5;241m.\u001b[39mtree_map(eqxi\u001b[38;5;241m.\u001b[39mnondifferentiable_backward, residual)\n",
      "    \u001b[0;31m[... skipping hidden 8 frame]\u001b[0m\n",
      "File \u001b[0;32m~/GP/env/icenode-dev/lib/python3.11/site-packages/optimistix/_ad.py:79\u001b[0m, in \u001b[0;36m_implicit_impl\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@eqx\u001b[39m\u001b[38;5;241m.\u001b[39mfilter_custom_jvp\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_implicit_impl\u001b[39m(fn_primal, fn_rewrite, inputs, tags, linear_solver):\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m fn_rewrite, tags, linear_solver\n\u001b[0;32m---> 79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fn_primal(inputs)\n",
      "File \u001b[0;32m~/GP/env/icenode-dev/lib/python3.11/site-packages/optimistix/_iterate.py:254\u001b[0m, in \u001b[0;36m_iterate\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    251\u001b[0m     _, _, state, _ \u001b[38;5;241m=\u001b[39m carry\n\u001b[1;32m    252\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m solver\u001b[38;5;241m.\u001b[39mbuffers(state)\n\u001b[0;32m--> 254\u001b[0m final_carry \u001b[38;5;241m=\u001b[39m while_loop(cond_fun, body_fun, init_carry, max_steps\u001b[38;5;241m=\u001b[39mmax_steps)\n\u001b[1;32m    255\u001b[0m final_y, num_steps, dynamic_final_state, final_aux \u001b[38;5;241m=\u001b[39m final_carry\n\u001b[1;32m    256\u001b[0m final_state \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mcombine(static_state, dynamic_final_state)\n",
      "File \u001b[0;32m~/GP/env/icenode-dev/lib/python3.11/site-packages/equinox/internal/_loop/loop.py:103\u001b[0m, in \u001b[0;36mwhile_loop\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m     99\u001b[0m     cond_fun_, body_fun_, init_val_, _ \u001b[38;5;241m=\u001b[39m common_rewrite(\n\u001b[1;32m    100\u001b[0m         cond_fun, body_fun, init_val, max_steps, buffers, makes_false_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    101\u001b[0m     )\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m cond_fun, body_fun, init_val\n\u001b[0;32m--> 103\u001b[0m     _, _, _, final_val \u001b[38;5;241m=\u001b[39m lax\u001b[38;5;241m.\u001b[39mwhile_loop(cond_fun_, body_fun_, init_val_)\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m final_val\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m kind \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheckpointed\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "File \u001b[0;32m~/GP/env/icenode-dev/lib/python3.11/site-packages/equinox/internal/_loop/common.py:463\u001b[0m, in \u001b[0;36mcommon_rewrite.<locals>.new_body_fun\u001b[0;34m(val)\u001b[0m\n\u001b[1;32m    461\u001b[0m step, pred, _, val \u001b[38;5;241m=\u001b[39m val\n\u001b[1;32m    462\u001b[0m buffer_val \u001b[38;5;241m=\u001b[39m _wrap_buffers(val, pred, tag)\n\u001b[0;32m--> 463\u001b[0m buffer_val2 \u001b[38;5;241m=\u001b[39m body_fun(buffer_val)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;66;03m# Needed to work with `disable_jit`, as then we lose the automatic\u001b[39;00m\n\u001b[1;32m    465\u001b[0m \u001b[38;5;66;03m# ArrayLike->Array cast provided by JAX's while loops.\u001b[39;00m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;66;03m# The input `val` is already cast to Array below, so this matches that.\u001b[39;00m\n\u001b[1;32m    467\u001b[0m buffer_val2 \u001b[38;5;241m=\u001b[39m jtu\u001b[38;5;241m.\u001b[39mtree_map(fixed_asarray, buffer_val2)\n",
      "File \u001b[0;32m~/GP/env/icenode-dev/lib/python3.11/site-packages/optimistix/_iterate.py:247\u001b[0m, in \u001b[0;36m_iterate.<locals>.body_fun\u001b[0;34m(carry)\u001b[0m\n\u001b[1;32m    244\u001b[0m new_y, new_state, aux \u001b[38;5;241m=\u001b[39m solver\u001b[38;5;241m.\u001b[39mstep(fn, y, args, options, state, tags)\n\u001b[1;32m    245\u001b[0m new_dynamic_state, new_static_state \u001b[38;5;241m=\u001b[39m eqx\u001b[38;5;241m.\u001b[39mpartition(new_state, eqx\u001b[38;5;241m.\u001b[39mis_array)\n\u001b[0;32m--> 247\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m eqx\u001b[38;5;241m.\u001b[39mtree_equal(static_state, new_static_state) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_y, num_steps \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m, new_dynamic_state, aux\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for (ds_name, (X, y)) in zip(['California',\n",
    "                             'Diabetes'\n",
    "                             ],  [(X_miss_california, y_miss_california), \n",
    "                                  (X_miss_diabetes, y_miss_diabetes)\n",
    "                                 ]):\n",
    "    for opt_name, optimizer in optax_optimisers.items():\n",
    "        model_name = 'JAX_MLP_ICNN'\n",
    "        for icnn_opt_name in ['adam', 'polyak_sgd', 'lamb', 'yogi']:\n",
    "            print(icnn_opt_name)\n",
    "            model = get_mlp_icnn_regressor(X, opt=optimizer, icnn_optax_optimiser_name=icnn_opt_name)\n",
    "            resicnn = get_imput_regress_scores_cv(model, X, y)\n",
    "            resicnn['Dataset'] = ds_name\n",
    "            resicnn['Model'] = model_name\n",
    "            res['Optimizer'] = opt_name\n",
    "            res['ICNN-Opt'] = icnn_opt_name\n",
    "    dfs.append(resicnn)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9083dbcf-763e-48c5-920f-a262e51fa746",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat(dfs)\n",
    "results['Method'] = results['Model'] + '-' + results['Imputer']\n",
    "results[(results.Dataset == 'California') & (results.split == 'TEST')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f06521-855a-46a9-b1a9-f7ab83779d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "sns.barplot(results[(results.Dataset == 'Diabetes') & (results.split == 'TEST')], \n",
    "            x=\"MSE\", y=\"Method\", errorbar=\"sd\"\n",
    "           )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3439194b-9c52-4feb-9b8a-dec561659ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "claifornia_imputer = ICNNObsDecoder(observables_size=X_miss_california.shape[1], state_size=0,  hidden_size_multiplier=3, depth=10, key=jrandom.PRNGKey(0))\n",
    "diabetes_imputer = ICNNObsDecoder(observables_size=X_miss_diabetes.shape[1], state_size=0,  hidden_size_multiplier=3, depth=10, key=jrandom.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e32931-6d99-4286-b059-12a62c61358d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22286c09-b6d5-48a2-ab66-4266844e9efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_scores.append(get_scores_for_icnn_imputer(claifornia_imputer,  X_miss_california, X_california))\n",
    "diabetes_scores.append(get_scores_for_icnn_imputer(diabetes_imputer,  X_miss_diabetes, X_diabetes))\n",
    "x_labels.append(\"ICNN Imputation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc80f18-e2c3-45ec-bd7c-9e029de6c792",
   "metadata": {},
   "outputs": [],
   "source": [
    "california_scores, california_scores_per_feature = zip(*california_scores)\n",
    "diabetes_scores, diabetes_scores_per_feature = zip(*diabetes_scores)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10f03300-e9ab-4b03-9f0b-44c02d584322",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame({'Diabetes': diabetes_scores, #'California': california_scores, \n",
    "                        'Method': x_labels})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3b34c3-e1ef-4a75-919c-b3f3d244fe6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_features = [f'feature_{i}' for i in range(len(california_scores_per_feature[0]))]\n",
    "diab_features = [f'feature_{i}' for i in range(len(diabetes_scores_per_feature[0]))]\n",
    "cal_features_res = pd.DataFrame(dict(zip(x_labels, california_scores_per_feature)), index=cal_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296101c7-22f2-439d-910d-ea3aa3998f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_features_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785d00a2-4dec-4fd2-9fb3-52a1b6ba9089",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
