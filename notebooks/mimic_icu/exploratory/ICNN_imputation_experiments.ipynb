{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94d59416-2f91-473a-8103-a3cd6343ac3d",
   "metadata": {},
   "source": [
    "## Libs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62c28bbb-0bea-4165-9bee-f54dad24ccc6",
   "metadata": {},
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom \n",
    "import equinox as eqx\n",
    "import optax\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "\n",
    "jax.config.update('jax_enable_x64', True)\n",
    "# jax.config.update('jax_check_tracer_leaks', True) \n",
    "sys.path.append(\"../../..\")\n",
    "from lib.ml.base_models import ICNNObsDecoder\n",
    "import lib.ehr.example_datasets.mimiciv_aki as m4aki\n",
    "from lib.ehr.tvx_ehr import TVxEHR\n",
    "from lib.utils import modified_environ, write_config\n",
    " "
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "bedd275f-c187-456f-9624-228e685e2531",
   "metadata": {},
   "source": [
    "## Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda298bc-79e7-4631-863f-23c451fb883e",
   "metadata": {},
   "source": [
    "### First Time Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8645d508-5ce0-4e81-a28b-b2b0dccf9644",
   "metadata": {},
   "source": [
    "# tvx = m4aki.TVxAKIMIMICIVDataset.load('/home/asem/GP/ehr-data/mimic4aki-cohort/tvx_aki.h5')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cbf73a-2b16-4e1a-9128-f2877a9b954d",
   "metadata": {},
   "source": [
    "# obs = [adm.observables  for subject in tvx0.subjects.values() for adm in subject.admissions]\n",
    "# adm_id = sum(([adm.admission_id] * len(adm.observables.time)  for subject in tvx0.subjects.values() for adm in subject.admissions), [])\n",
    "# subj_id = sum(([subject.subject_id] * len(adm.observables.time)  for subject in tvx0.subjects.values() for adm in subject.admissions), [])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d25b12a0-f4ed-40dd-bec6-9fef0e79caa6",
   "metadata": {},
   "source": [
    "# obs_val = np.vstack([obs_i.value for obs_i in obs])\n",
    "# obs_mask = np.vstack([obs_i.mask for obs_i in obs])\n",
    "# obs_time = np.hstack([obs_i.time for obs_i in obs])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09c8afd5-475a-4a92-b73d-6e66243662cc",
   "metadata": {},
   "source": [
    "# tvx0.scheme.obs\n",
    "# features = list(map(tvx0.scheme.obs.desc.get, tvx0.scheme.obs.codes))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5b3e1440-1b1c-4fe6-806c-1839440ee716",
   "metadata": {},
   "source": [
    "# obs_val = pd.DataFrame(obs_val, columns=features)\n",
    "# obs_mask = pd.DataFrame(obs_mask.astype(int), columns=features)\n",
    "# meta = pd.DataFrame({'subject_id': subj_id, 'admission_id': adm_id, 'time': obs_time})\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a784503e-6175-43ab-8522-58447cf5e222",
   "metadata": {},
   "source": [
    "# artificial_mask = obs_mask.copy()\n",
    "# artificial_mask = obs_mask & np.array(jrandom.bernoulli(jrandom.PRNGKey(0), p=0.5, shape=obs_mask.shape))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "28e3b288-d34d-4a43-8f79-98c5965ddb03",
   "metadata": {},
   "source": [
    "# obs_val.to_csv('missingness_data/missingness_vals.csv')\n",
    "# obs_mask.to_csv('missingness_data/missingness_mask.csv')\n",
    "# meta.to_csv('missingness_data/meta.csv')\n",
    "# artificial_mask.to_csv('missingness_data/missingness_artificial_mask.csv')\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "808916f3-e3ff-4239-a26b-d34631abd624",
   "metadata": {},
   "source": [
    "### Later Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a9b42f56-270f-4e17-8c93-e79af7638e72",
   "metadata": {},
   "source": [
    "obs_val = pd.read_csv('missingness_data/missingness_vals.csv', index_col=[0])\n",
    "obs_mask = pd.read_csv('missingness_data/missingness_mask.csv', index_col=[0])\n",
    "artificial_mask = pd.read_csv('missingness_data/missingness_artificial_mask.csv', index_col=[0])\n",
    "meta = pd.read_csv('missingness_data/meta.csv', index_col=[0])\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "1e678fa0-e9a7-4120-aba9-5c17792af3d7",
   "metadata": {},
   "source": [
    "### Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed9fabd6-6809-460f-a454-9572153c8299",
   "metadata": {},
   "source": [
    "split_ratio = 0.7\n",
    "seed = 0\n",
    "indices = jrandom.permutation(jrandom.PRNGKey(seed), len(obs_val))\n",
    "train_idx = indices[:int(split_ratio * len(indices))]\n",
    "test_idx = indices[int(split_ratio * len(indices)):]\n",
    "\n",
    "obs_val_train = jnp.array(obs_val.iloc[train_idx].to_numpy())\n",
    "obs_mask_train = jnp.array(obs_mask.iloc[train_idx].to_numpy())\n",
    "art_mask_train =  jnp.array(artificial_mask.iloc[train_idx].to_numpy())\n",
    "\n",
    "obs_val_test = jnp.array(obs_val.iloc[test_idx].to_numpy())\n",
    "obs_mask_test = jnp.array(obs_mask.iloc[test_idx].to_numpy())\n",
    "art_mask_test =  jnp.array(artificial_mask.iloc[test_idx].to_numpy())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c89b5fc4-7103-4100-84fd-d689ada680b0",
   "metadata": {},
   "source": [
    "## Model Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3439194b-9c52-4feb-9b8a-dec561659ffd",
   "metadata": {},
   "source": [
    "model = ICNNObsDecoder(observables_size=obs_mask.shape[1], state_size=0, \n",
    "                       hidden_size_multiplier=3, depth=8, key=jrandom.PRNGKey(0))\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d9d3c31-e0fa-4ceb-b229-f06343a21a91",
   "metadata": {},
   "source": [
    "model"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a55b5878-bc25-4ba8-8a8a-4f4482efb1ab",
   "metadata": {},
   "source": [
    "model.f_energy"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "f3e32931-6d99-4286-b059-12a62c61358d",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10e8590f-0cb5-4878-984c-b467b3d7c0e5",
   "metadata": {},
   "source": [
    "def mse(X, X_hat, M, axis=None):\n",
    "    return jnp.mean((X - X_hat)**2, where=M, axis=axis)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def loss(model, batch_X, batch_M, batch_M_art):\n",
    "    # Zero for artificially missig values\n",
    "    batch_X_art = jnp.where(batch_M_art, batch_X, 0.)\n",
    "    # Tune for artificially masked-out values, fix mask-in (batch_M_art) values.\n",
    "    X_imp, aux = eqx.filter_vmap(model.partial_input_optimise)(batch_X_art, batch_M_art)\n",
    "    # Penalise discrepancy with artifially masked-out values.\n",
    "    return mse(batch_X, X_imp, (~batch_M_art) & batch_M), aux\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def zero_imputer_loss(batch_X, batch_M, batch_M_art):\n",
    "    return mse(batch_X, jnp.zeros_like(batch_X), (~batch_M_art) & batch_M)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def weighted_loss(model, batch_X, batch_M, batch_M_art, weights):\n",
    "    # Zero for artificially missig values\n",
    "    batch_X_art = jnp.where(batch_M_art, batch_X, 0.)\n",
    "    # Tune for artificially masked-out values, fix mask-in (batch_M_art) values.\n",
    "    X_imp, aux = eqx.filter_vmap(model.partial_input_optimise)(batch_X_art, batch_M_art)\n",
    "    # Penalise discrepancy with artifially masked-out values.\n",
    "    loss_vec = mse(batch_X, X_imp, (~batch_M_art) & batch_M, axis=0)\n",
    "    unweighted_loss = mse(batch_X, X_imp, (~batch_M_art) & batch_M)\n",
    "    weighted_loss_ = jnp.nansum(loss_vec * weights) / jnp.sum(weights) \n",
    "    return weighted_loss_, unweighted_loss, aux\n",
    "    \n",
    "@eqx.filter_value_and_grad(has_aux=True)\n",
    "def loss_grad(model, batch_X, batch_M, batch_M_art):\n",
    "    return loss(model, batch_X, batch_M, batch_M_art)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, opt_state, batch_X, batch_M, batch_M_art):\n",
    "    (loss, aux), grads = loss_grad(model, batch_X, batch_M, batch_M_art)\n",
    "    updates, opt_state = optim.update(grads, opt_state, params=model)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return (loss, aux), model, opt_state\n",
    "\n",
    "def dataloader(arrays, batch_size, *, key):\n",
    "    dataset_size = arrays[0].shape[0]\n",
    "    indices = jnp.arange(dataset_size)\n",
    "    while True:\n",
    "        perm = jrandom.permutation(key, indices)\n",
    "        (key,) = jrandom.split(key, 1)\n",
    "        start = 0\n",
    "        end = batch_size\n",
    "        while end < dataset_size:\n",
    "            batch_perm = perm[start:end]\n",
    "            yield tuple(array[batch_perm] for array in arrays)\n",
    "            start = end\n",
    "            end = start + batch_size\n",
    "\n",
    "\n",
    "lr=1e-3\n",
    "steps=1000000\n",
    "train_batch_size=256\n",
    "test_batch_size=512\n",
    "# train_batch_size=1\n",
    "# test_batch_size=1\n",
    "eval_frequency = 10\n",
    "\n",
    "optim = optax.adadelta(lr)\n",
    "opt_state = optim.init(eqx.filter(model, eqx.is_inexact_array))\n",
    "data_train = (obs_val_train, obs_mask_train, art_mask_train)\n",
    "data_test = (obs_val_test, obs_mask_test, art_mask_test)\n",
    "\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ede7d5-089b-47b2-828e-06ecfe710bda",
   "metadata": {},
   "source": [
    "train_batches = dataloader(data_train, train_batch_size, key=jrandom.PRNGKey(0))\n",
    "test_batches = iter(dataloader(data_test, train_batch_size, key=jrandom.PRNGKey(0)))\n",
    "\n",
    "progress = tqdm(range(steps))\n",
    "train_history = defaultdict(list)\n",
    "test_history = defaultdict(list)\n",
    "\n",
    "for step, batch_train in zip(progress, train_batches):\n",
    "    start = time.time()\n",
    "    (train_loss, train_aux), model, opt_state = make_step(model, opt_state, *batch_train)\n",
    "    train_zloss =  zero_imputer_loss(*batch_train)\n",
    "    train_nsteps = int(sum(train_aux.n_steps) / len(train_aux.n_steps))\n",
    "    train_history['zloss'].append(train_zloss)\n",
    "    train_history['loss'].append(train_loss)\n",
    "    train_history['n_opt_steps'].append(train_nsteps)\n",
    "    \n",
    "    end = time.time()\n",
    "    if (step % eval_frequency) == 0 or step == steps - 1:\n",
    "        batch_test = next(test_batches)\n",
    "        test_wloss, test_loss, aux = weighted_loss(model, *batch_test, weights=art_mask_train.mean(axis=0))\n",
    "        test_zloss = zero_imputer_loss(*batch_test)\n",
    "        nsteps = int(sum(aux.n_steps) / len(aux.n_steps))\n",
    "        test_history['zloss'].append(test_zloss)\n",
    "        test_history['loss'].append(test_loss)\n",
    "        test_history['wloss'].append(test_wloss)\n",
    "        test_history['n_opt_steps'].append(nsteps)\n",
    "        \n",
    "    progress.set_description(f\"Trn-L: {train_loss:.3f}, Trn-Z-L: {train_zloss: .3f}, Tst N-steps: {train_nsteps}, \" \n",
    "                             f\"Tst-L: {test_loss:.3f}, Tst-W-L: {test_wloss:.3f}, Tst-Z-L: \"\n",
    "                             f\"{test_zloss:.3f}, Tst N-steps: {nsteps}, Computation time: {end - start:.2f}, \")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da92e8-d693-4cb7-b31e-bfcd368dd429",
   "metadata": {},
   "source": [
    "train_stats = pd.DataFrame(train_history)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78844f69-101e-4089-8bc6-bdd36b99c742",
   "metadata": {},
   "source": [
    "(train_stats.zloss > train_stats.loss).mean()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b7bd88-323c-4716-959a-7d2423c7fc85",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
